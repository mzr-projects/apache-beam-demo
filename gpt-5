Below is a concise, production-ready way to run your Apache Beam pipeline on Apache Flink.

Pick your approach

-  FlinkRunner (Java, recommended for your Java-only pipeline)
  - You run your fat JAR with --runner=FlinkRunner and point it at a running Flink cluster via --flinkMaster.
-  Portable Flink Runner (JobServer)
  - Useful when you need cross-language transforms or a central JobServer. You run a Flink JobServer and submit jobs to it. If you don’t need this, prefer FlinkRunner.

Prereqs

-  A running Flink cluster (local or remote). For local dev:
  - Download Flink, then run: bin/start-cluster.sh
  - Web UI: http://localhost:8081
-  Use compatible Beam and Flink versions (check Beam’s Flink Runner compatibility matrix for your Beam version).
-  Kafka and Elasticsearch reachable from Flink TaskManagers.

Maven dependencies (POM)

Use the Flink runner that matches your Flink. Example below (adjust versions to your stack and the compatibility matrix).

```xml
<properties>
  <beam.version>2.5x.0</beam.version>
  <!-- Match the minor version suffix with your Flink: -1.16, -1.17, etc. -->
</properties>

<dependencies>
  <!-- Beam core -->
  <dependency>
    <groupId>org.apache.beam</groupId>
    <artifactId>beam-sdks-java-core</artifactId>
    <version>${beam.version}</version>
  </dependency>

  <!-- Flink Runner (choose the one matching your Flink minor version) -->
  <dependency>
    <groupId>org.apache.beam</groupId>
    <artifactId>beam-runners-flink-1.17</artifactId>
    <version>${beam.version}</version>
  </dependency>

  <!-- IOs -->
  <dependency>
    <groupId>org.apache.beam</groupId>
    <artifactId>beam-sdks-java-io-kafka</artifactId>
    <version>${beam.version}</version>
  </dependency>
  <dependency>
    <groupId>org.apache.beam</groupId>
    <artifactId>beam-sdks-java-io-elasticsearch</artifactId>
    <version>${beam.version}</version>
  </dependency>

  <!-- Your JSON/ES client libs if used in custom DoFns, etc. -->
  <!-- Add others as needed -->
</dependencies>

<build>
  <plugins>
    <!-- Create a single, shaded (fat) JAR to avoid dependency hell on Flink -->
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-shade-plugin</artifactId>
      <version>3.5.0</version>
      <executions>
        <execution>
          <phase>package</phase>
          <goals><goal>shade</goal></goals>
          <configuration>
            <createDependencyReducedPom>false</createDependencyReducedPom>
            <relocations>
              <!-- Optional but recommended to avoid conflicts: relocate JSON/HTTP clients you bundle -->
              <!--
              <relocation>
                <pattern>com.fasterxml.jackson</pattern>
                <shadedPattern>shadow.com.fasterxml.jackson</shadedPattern>
              </relocation>
              -->
            </relocations>
            <filters>
              <filter>
                <artifact>*:*</artifact>
                <excludes>
                  <exclude>META-INF/*.SF</exclude>
                  <exclude>META-INF/*.DSA</exclude>
                  <exclude>META-INF/*.RSA</exclude>
                </excludes>
              </filter>
            </filters>
          </configuration>
        </execution>
      </executions>
    </plugin>
  </plugins>
</build>
```

Enable Flink runner options in code

Add FlinkPipelineOptions in your main and set streaming, parallelism, and checkpointing (example):

```java
import org.apache.beam.runners.flink.FlinkPipelineOptions;
import org.apache.beam.runners.flink.FlinkRunner;

public class KafkaToElasticsearch {
  public static void main(String[] args) {
    PipelineOptions options =
        PipelineOptionsFactory.fromArgs(args).withValidation().create();

    // Streaming
    options.as(StreamingOptions.class).setStreaming(true);

    // Flink-specific options
    FlinkPipelineOptions fopts = options.as(FlinkPipelineOptions.class);
    fopts.setRunner(FlinkRunner.class);
    // Point to your Flink JobManager; for local: "localhost:8081"
    fopts.setFlinkMaster("localhost:8081");
    fopts.setParallelism(8);                      // tune for your cluster
    fopts.setMaxParallelism(256);                 // optional cap
    fopts.setCheckpointingInterval(10_000L);      // 10s checkpoints
    fopts.setCheckpointTimeoutMillis(10 * 60_000L);
    // Optional: fopts.setShutdownSourcesOnFinalWatermark(true);
    // Optional: fopts.setNumberOfExecutionRetries(3);

    Pipeline p = Pipeline.create(options);

    // ... your existing pipeline graph (KafkaIO -> batched ES lookup -> ElasticsearchIO)

    p.run();
  }
}
```

Run locally against a local Flink cluster

1) Start Flink:
-  bin/start-cluster.sh
-  Verify UI at http://localhost:8081

2) Build your fat JAR:
-  mvn -DskipTests package

3) Run the pipeline (the job will be submitted to Flink via the FlinkRunner):
```bash
java -jar target/your-pipeline-jar-with-dependencies.jar \
  --runner=FlinkRunner \
  --flinkMaster=localhost:8081 \
  --parallelism=8 \
  --checkpointingInterval=10000 \
  --kafkaBootstrap=localhost:9092 \
  --kafkaTopic=input-topic \
  --esHosts=localhost:9200 \
  --esOutputIndex=enriched-events \
  --lookupBatchSize=200 \
  --lookupMaxBufferSeconds=2 \
  --lookupParallelism=64
```

Run on a remote Flink cluster (standalone, YARN, or Kubernetes)

-  Make sure the Flink JobManager RPC address/port is reachable. The same command works; just set --flinkMaster to the JobManager’s REST address (e.g., jobmanager:8081 on K8s).
-  Configure Flink’s checkpoint/state backends in Flink’s conf (recommended for prod):
  - conf/flink-conf.yaml
    - state.checkpoints.dir: s3://… or hdfs://…
    - state.backend: rocksdb
    - high-availability: org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory (or ZK for standalone)
-  Use savepoints for upgrades:
  - bin/flink savepoint <jobId>
  - bin/flink cancel -s <path> <jobId>
  - Relaunch the new JAR with the savepoint path via --savepointPath (Beam passes it through via Flink options).

Submitting via Flink CLI (optional)

You can also run your fat JAR with Flink’s CLI, which executes your main() on-cluster and your code still submits to the JobManager via FlinkRunner:

```bash
flink run -c com.yourorg.KafkaToElasticsearch \
  target/your-pipeline-jar-with-dependencies.jar \
  --runner=FlinkRunner \
  --flinkMaster=jobmanager:8081 \
  --parallelism=16 \
  ...your app args...
```

Portable Flink Runner (JobServer) quick outline

-  Start a Flink JobServer (Docker or jar). Example (Docker; adjust tags):
  - docker run -p 8099:8099 apache/beam_flink1.17_job_server:2.5x.0
-  Submit your pipeline with --runner=PortableRunner and --jobEndpoint=host:8099 (you also provide --environmentType=PROCESS or DOCKER).
-  This is more moving parts; stick to FlinkRunner unless you specifically need cross-language.

Operational tips

-  Bump Kafka partitions to match/drive parallelism.
-  In Elasticsearch, make writes idempotent (deterministic IDs) to tolerate at-least-once.
-  Tune:
  - Flink parallelism vs. Kafka partitions
  - CheckpointingInterval (5–30 s typical), and RocksDB state backend for large state
  - ElasticsearchIO bulk: ~1,000 docs or ~5–10 MB payloads per request
-  Watch Flink UI metrics for backpressure; scale TaskManagers/parallelism accordingly.

If you share your exact Beam and Flink versions (and where you’re running Flink: local, K8s, YARN), I can give you a copy-paste set of dependency coordinates and the exact command to run.

